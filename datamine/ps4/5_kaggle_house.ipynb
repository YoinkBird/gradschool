{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b0f673b73b7d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b0f673b73b7d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    data read\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recreating: https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models/notebook\n",
    "# (not forking, as the goal is to use it as a tutorial)\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test  = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "# contains 'SalePrice', which not in test\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start and stop features. everything except for 'id' and 'saleprice'\n",
    "feat_s = 'MSSubClass'\n",
    "feat_e = 'SaleCondition'\n",
    "# http://pandas.pydata.org/pandas-docs/stable/merging.html \n",
    "# two options. note that 'train' has one extra column 'SalePrice' which must be omitted\n",
    "# creating a tuple of dataframes\n",
    "df_total1 = pd.concat((train.loc[:,feat_s:feat_e], test.loc[:,feat_s:feat_e]))\n",
    "print(df_total1.shape)\n",
    "# or just taking the whole thing and dropping what we don't want\n",
    "df_total2 = pd.concat([train.drop(['SalePrice'],axis=1),test])\n",
    "df_total2.drop(['Id'], axis=1, inplace=True)\n",
    "print(df_total2.shape)\n",
    "# compare the two to verify that this worked correctly\n",
    "#print(np.alltrue(df_total1.columns == df_total.columns))\n",
    "#print(np.alltrue(df_total1.index == df_total.index))\n",
    "# print(df_total2.head())\n",
    "# print(df_total1.head())\n",
    "# not \"equal\", but likely just because of NaN. regardless, not taking any risks.\n",
    "# df_total2 == df_total1\n",
    "# use original approach\n",
    "alldf = df_total1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preproc\n",
    "\n",
    "kaggle deleted my progress, going to re-create really fast and without documentation\n",
    "\n",
    "- normalise: visualisation\n",
    "- normalise: xfrm skewed num feats with log(feat + 1)\n",
    "- dummy vars\n",
    "- s/NaN/mean()/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalise - visualisation\n",
    "bigimg = 0\n",
    "if(bigimg == 1):\n",
    "    matplotlib.rcParams['figure.figsize'] = (12.0,6.0)\n",
    "# tmp df for visualisation\n",
    "prices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log transform target\n",
    "# TODODOC: np.log1p\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# log transform skewed numeric features\n",
    "# generate a selector # TODO: does it have to be with '!=object' or can that one pd data type be used?\n",
    "num_feats = alldf.dtypes[alldf.dtypes != \"object\"].index\n",
    "\n",
    "# compute skew\n",
    "# generate selector:\n",
    "# TODODOC; doc for scipi.stats.skew (np and wiki)\n",
    "skewed_feats = train[num_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75] # TODO: why?\n",
    "skew_idx = skewed_feats.index\n",
    "\n",
    "alldf[skew_idx] = np.log1p(alldf[skew_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_dummies - essentially: convert non-numeric features to binary series\n",
    "# expand features with non-numeric into multiple columns\n",
    "# TODO:  e.g. SaleCondition\n",
    "# original values: # note: this is the wrong way to \"uniqify\"!# pd.get_dummies(alldf['SaleCondition']).columns\n",
    "# Index(['Abnorml', 'AdjLand', 'Alloca', 'Family', 'Normal', 'Partial'], dtype='object')\n",
    "# after get_dummies on entire df:\n",
    "# ['SaleCondition_Abnorml', 'SaleCondition_AdjLand','SaleCondition_Alloca', 'SaleCondition_Family', 'SaleCondition_Normal','SaleCondition_Partial']\n",
    "alldf = pd.get_dummies(alldf)\n",
    "print(\"after get_dummies: \",alldf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# - s/NaN/mean()/\n",
    "tmpdf = alldf.fillna(alldf.mean())\n",
    "alldf.fillna(alldf.mean(),inplace=True)\n",
    "# verify that 'inplace' works:\n",
    "np.alltrue(tmpdf == alldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models\n",
    "- create matrices for sklearn\n",
    "- regularizations:\n",
    "    - l1_lasso\n",
    "    - l2_ridge\n",
    "- create function for calc of RMSE\n",
    "\n",
    "TODODOC:\n",
    "\n",
    "add all the links about sklearn model_selection and cross_val_score\n",
    "\n",
    "basically: for cross_val_score, higher scores are, like, the best,\n",
    "so the retval of neg_mean_squared_error is negative because it's, like, the worst, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create matrices for sklearn\n",
    "# TODO: learn and understand the selector syntax. I think this is selecting all until 1460 and all from 1460\n",
    "print(train.shape)\n",
    "X_train = alldf[:train.shape[0]] # 1460\n",
    "X_test  = alldf[train.shape[0]:]\n",
    "y = train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODODOC\n",
    "def rmse_cv(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# L2 Ridge\n",
    "model_ridge = Ridge()\n",
    "\n",
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5, 7,8,9, 10, 11, 12, 15, 30, 50, 75]\n",
    "#alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\n",
    "\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation - Just DÃ¼ It\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "\n",
    "# random\n",
    "model_ridge = RidgeCV(alphas = alphas).fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get min value\n",
    "# ref: 0.1273373466867076\n",
    "cv_ridge.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# L1 Lasso\n",
    "# Lasso does feature selection - setting coefficients of features it deems unimportant to zero\n",
    "alphas_l1 = [1, 0.1, 0.001, 0.0005]\n",
    "model_lasso = LassoCV(alphas = alphas_l1).fit(X_train, y)\n",
    "# ref: 0.12314421090977427\n",
    "print(rmse_cv(model_lasso).mean())\n",
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso picked %s variables and eliminated the other %s variables\" % (sum(coef != 0),sum(coef == 0)))\n",
    "# TODO:\n",
    "# Good job Lasso. One thing to note here however is that the features selected are not necessarily the \"correct\" ones \n",
    "# - especially since there are a lot of collinear features in this dataset. \n",
    "# One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is.\n",
    "print(\"Important Coefficients\")\n",
    "# matplotlib.rcParams\n",
    "imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\n",
    "imp_coef.plot(kind=\"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")\n",
    "# NOTE: most imp pos: GrLivArea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# residuals:\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0,6.0) # TODO: determine how to have the dots with a black border\n",
    "preds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\n",
    "preds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\n",
    "preds.plot(x = \"preds\", y = \"residuals\", kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - isntall on windows, somehow\n",
    "#import xgboost as xgb\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert back to non-log\n",
    "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
    "# TODO: use xgb instead of ridge\n",
    "ridge_preds = np.expm1(model_ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\"ridge\":ridge_preds,\"lasso\":lasso_preds})\n",
    "predictions.plot(x = \"ridge\", y = \"lasso\", kind = \"scatter\")\n",
    "\n",
    "# predictions = pd.DataFrame({\"xgb\":xgb_preds,\"lasso\":lasso_preds})\n",
    "# predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
